\chapter{Supervised Sequence to Sequence Learning}
This chapter provides background material and literature review of supervised sequence to sequence learning. Section 2.1 introduces supervised learning in general and section 2.2 introduces the problem of sequence to sequence learning in a supervised setting with examples from various domains like Machine Translation, Protein Secondary Structure Prediction and POS-Tagging.

\section{Supervised Learning}
Here, the goal is to learn a mapping from inputs $x$ to outputs $y$, given a labeled set of input-output pairs $D = \left \{ \left (x_{i}, y_{i} \right ) \right \}_{i=1}^{N}$. Here $D$ is called the training set, and $N$ is the number of training examples.

In the simplest setting, each training input $x_{i}$ is a $D$-dimensional vector of numbers, representing, say, the height and weight of a person. These are called \textbf{features}, \textbf{attributes} or \textbf{covariates}. In general, however, $x_{i}$ could be a complex structured object, such as an image, a sentence, an email message, a time series, a molecular shape, a graph, etc.

Similarly the form of the output or \textbf{response variable} can in principle be anything, but most methods assume that $y_{i}$ is a \textbf{categorical} or \textbf{nominal} variable from some finite set, $y_{i} \in \left \{1, \ldots, C \right \}$ (such as male or female), or that $y_{i}$ is a real-valued scalar (such as income level). When $y_{i}$ is categorical, the problem is known as \textbf{classification} or \textbf{pattern recognition}, and when $y_{i}$ is real-valued, the problem is known as \textbf{regression}. Another variant, known as \textbf{ordinal regression}, occurs where label space $Y$ has some natural ordering, such as grades A-F.

\section{Sequence to Sequence Learning}
Here, the goal is to learn a mapping from sequence of inputs $\left \{x_{1}, x_{2}, \ldots, x_{T} \right \}$ to a sequence of outputs $\left \{y_{1}, y_{2}, \ldots, y_{T} \right \}$, given a labeled set of input-output sequence pairs $D = \left \{ \left ( \left \{x^{i}_{1}, x^{i}_{2}, \ldots, x^{i}_{T} \right \}, \left \{y^{i}_{1}, y^{i}_{2}, \ldots, y^{i}_{T} \right \} \right ) \right \}_{i=1}^{N}$. Here $D$ is called the training set, and $N$ is the number of training examples.