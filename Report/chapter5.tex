\chapter{Neural Turing Machine}
The combination of neural networks, specifically recurrent neural networks, with a large addressable memory, which they can interact with by attentional processes, is called a Neural Turing Machine due to its analogy to Turing Machine with infinite memory tape. Unlike Turing Machine, an NTM is a differentiable computer that can be trained by gradient descent, yielding a practical mechanism for learning programs. \cite{ntm} Preliminary results show that an NTM can infer simple algorithms such as copying, sorting and associative recall.

\section{Architecture}
A Neural Turing Machine architecture contains two basic components: a neural network controller and a memory bank. Fig. \ref{fig:ntm_arch} presents a basic diagram of the NTM architecture. The controller not only interacts with an external world via input and output vectors but also interacts with a memory matrix using selective read and write operations. By analogy to the Turing machine, network outputs that parametrise these operations are referred as ``heads".
Note that Turing Machine has a defined controller in terms of the task that the machine has to perform. On the other hand, the controller in Neural Turing Machine is learnt based on the input output pairs corresponding to a particular task.

Breifly, the controller in NTM is a neural network (feedforward, recurrent or LSTM), the memory is a matrix consisting of $N$ memory slots each of an appropriate size $M$, the read and write heads are vectors of length $N$ used by the controller to strongly focus on one memory slot or weakly focus on multiple memory slots.

\section{Attention Mechanism}
The process or mechanism with which the controller interacts with the memory is called attention mechanism. In NTM, the attention mechanism conmprises of procedures to read from memory, write into memory and updation of read and write heads.

\subsection{Reading from Memory}
Let $\textbf{M}_t$ be the contents of the \textit{N}x\textit{M} memory matrix at time t, where \textit{N} is the number of memory locations and \textit{M} is the vector size at each location. Let $w_t$ be a vector of weightings over the \textit{N} locations emitted by a read head at time t. Since all the weightings are normalised, the \textit{N} elements $w_t$(i) of $\textbf{w}_t$ are given by:

\begin{equation} 
\sum_{i}^{} w_t(i)=1
\end{equation}

\noindent
The length \textit{M} read vector $\textbf{r}_t$ returned by the head is defined as a convex combination of the row-vectors $\textbf{M}_t$(i) in memory:
\begin{equation}
\textbf{r}_t \leftarrow \sum_{i}^{} w_t(i)\textbf{M}_t(i)
\end{equation}
\noindent
$\textbf{r}_t$ is differentiable with respect to both the memory and the weighting. The derivative of $\textbf{r}_t$ with respect to $\textbf{w}_t$ is:
\begin{equation}
\frac{\partial r_t(i)}{\partial w_t(j)} = \textbf{M}_t(i,j)
\end{equation}
\noindent
and with respect to $\textbf{M}_t$ is:
\begin{equation}
\frac{\partial r_t(k)}{\partial \textbf{M}_t(i,j)} = w_t(i)\mathbb{I}(k=j)
\end{equation}

\subsection{Writing into Memory}
Given a weighting $\textbf{w}_t$ emitted by a write head at time t, along with an erase vector $\textbf{e}_t$ whose \textit{M} elements all lie in the range (0,1), the memory vectors $\textbf{M}_{t-1}$(i) from the previous time-step are modified as follows: 
\begin{equation}
\widetilde{\textbf{M}_t}(i) \leftarrow \textbf{M}_{t-1}(i)[1-w_t(i)\textbf{e}_t]
\end{equation}  
\noindent
where \textbf{1} is a row vector of all 1-s, and the multiplication against memory location acts point-wise. Therefore, the elements of a memory location are reset to zero only if both the weighting at the location and the erase element are one; if either the weightings or the erase is zero, the memory is left unchanged. When multiple write heads are present, the erasures
can be performed in any order, as multiplication is commutative.

Note that $\widetilde{\textbf{M}}_t$ is differentiable with respect to $\textbf{M}_t$ $\textbf{w}_t$ and $\textbf{e}_t$ and the corresponding derivatives are:

\begin{equation}
\frac{\partial \widetilde{\textbf{M}}_t(i,j)}{\partial \textbf{M}_t(i',j')} = (1-w_t(i)e_t(j))\mathbb{I}(i=i')\mathbb{I}(j=j')
\end{equation}

\begin{equation}
\frac{\partial \widetilde{\textbf{M}}_t(i,j)}{\partial w_t(k)} = -\textbf{M}_{t-1}(i,j)e_t(j)\mathbb{I}(i=k)
\end{equation}

\begin{equation}
\frac{\partial \widetilde{\textbf{M}}_t(i,j)}{\partial e_t(k)} = -\textbf{M}_{t-1}(i,j)w_t(i)\mathbb{I}(k=j)
\end{equation}

Each write head also produces a length M add vector $\textbf{a}t$, which is added to the memory after the erase step has been performed:
\begin{equation}
\textbf{M}_t(i) \leftarrow \widetilde{\textbf{M}_t}(i) + w_t(i)\textbf{a}_t
\end{equation}
\noindent
Once again, the order in which the adds are performed by multiple heads is irrelevant. The combined erase and add operations of all the write heads produces the final content of the memory at time t.

Note that $\textbf{M}_t$ is differentiable with respect to $\textbf{w}_t$, $\textbf{a}_t$ and $\textbf{e}_t$ and the corresponding derivatives are:

\begin{equation}
\frac{\partial \textbf{M}_t(i,j)}{\partial \widetilde{\textbf{M}}_t(i',j'))} = \mathbb{I}(i=i')\mathbb{I}(j=j')
\end{equation}

\begin{equation}
\frac{\partial \textbf{M}_t(i,j)}{\partial w_t(k))} = w_t(i)\mathbb{I}(j=k)
\end{equation}

\begin{equation}
\frac{\partial \textbf{M}_t(i,j)}{\partial w_t(k))} = a_t(j)\mathbb{I}(i=k)
\end{equation}

\subsection{Addressing Mechanism}
The addressing mechanism used to update weightings is presented as a flow diagram in Fig. \ref{fig:addrmech}. The weightings arise by combining two addressing mechanisms: ``content-based addressing" and ``location-based addressing". The content based addressing focuses attention on locations based on similarity between current values in memory matrix and the values emitted by the controller. The location based addressing is used to facilitate both simple iterations across the locations of the memory and random-access jumps. It includes the interpolation, convolutional shift and finally, sharpening. The following sub-sections provides a description of each stage.

\begin{figure}[hH]
\centering
\includegraphics[width=1\textwidth]{6}
\caption{\textbf{Flow Diagram of Addressing Mechanism.\cite{ntm}} The key vector, $\textbf{k}_t$ and key strength, $\beta_t$ are used to perform content-based addressing of the memory matrix, $\textbf{M}_t$. The resulting content-based weighting is interpolated with the weighting from the previous time step
based on the value of the interpolation gate, $g_t$. The shift weighting, $s_t$ determines whether and by how much the weighting is rotated. Finally, depending on $\gamma_t$, the weighting is sharpened and used for memory access.}
\end{figure}

\subsubsection{Content Addressing}
The weightings produced by the content addressing module will focus on those memory slots $\textbf{M}_t$(i) which are similar to the length \textit{M} key vector $\textbf{k}_t$ (with respect to cosine similarity) and hence the content of the read vector (if formed by this weighting alone) will be similar to the content of the focused memory slot content. The precision of focus can be attenuated or amplified with a positive key strength $\beta_t$.

\noindent As an example, suppose memory looks like ${[A,B,C,D,...,Z]}$, 26 memory slots with one unique alphabetic character each, key vector is ${D}$ and key strength is very large (note that infinite key strength changes softmax to max). Then, the weighting produced by content addressing module will approximately be ${[0,0,0,1,...,0]}$ and corresponding read vector will be ${D}$.

\begin{equation}
w_t^c(i) \leftarrow \frac{\exp{(\beta_t\textbf{K}[\textbf{k}_t,\textbf{M}_t(i)])}}{\sum_{j}{} \exp{(\beta_t\textbf{K}[\textbf{k}_t,\textbf{M}_t(j)])}}
\end{equation}
\noindent
The cosine similarity measure is defined as:
\begin{equation}
\textbf{K}[\textbf{u},\textbf{v}] \leftarrow \frac{\textbf{u}.\textbf{v}}{\left \| \textbf{u} \right \|\left \| \textbf{v} \right \|}
\end{equation}
\noindent
Note that the derivative of $\textbf{w}_t^c$ with respect to $\beta_t$, $\textbf{k}_t$ and  $\textbf{M}_t$ are:
\begin{equation}
\frac{\partial w_t^c(i)}{\partial \beta_t} \leftarrow \frac{\exp{(\beta_tc_i)}(\sum_{j} (c_i-c_j)\exp{(\beta_tc_j)})}{(\sum_{j} \exp{(\beta_tc_j)})^2}
\end{equation}
\noindent
where
\begin{equation}
c_i \leftarrow \textbf{K}[\textbf{k}_t,\textbf{M}_t(i)]
\end{equation}
\noindent
Also,
\begin{equation}
\frac{w_t^c(i)}{\partial c_i} \leftarrow \frac{\sum_{j\neq i} \beta_t\exp{(\beta_tc_j)}}{(\sum_{j} \exp{(\beta_tc_j)})^2}
\end{equation}
\noindent
and
\begin{equation}
\frac{w_t^c(i)}{\partial c_j} \leftarrow \frac{-\beta_t\exp{(\beta_tc_i)}\exp{(\beta_tc_j)}}{(\sum_{j} \exp{(\beta_tc_j)})^2}
\end{equation}
\noindent
where
\begin{equation}
\frac{\partial \textbf{K[\textbf{u},\textbf{v}]}}{ \partial u(i)} \leftarrow \frac{1}{\left \| \textbf{u} \right \|\left \| \textbf{v} \right \|}\left [ v(i) - \frac{u(i)(\textbf{u}.\textbf{v})}{\left \| \textbf{u} \right \|^2} \right ]
\end{equation}
\noindent
where $\textbf{u}$=$\textbf{k}_t$ and $\textbf{v}$=$\textbf{M}_t(i)$. Now, one can use chain rule to find the derivative of  $\textbf{w}_t^c$ with respect to $\textbf{k}_t$ and  $\textbf{M}_t$.

\subsubsection{Interpolation}
Each head emits a scalar interpolation gate $g_t$ in the range (0,1). The value of $g_t$ is used to blend between the weighting $\textbf{w}_{t-1}$ produced by the head at the previous time-step and the weighting $\textbf{w}_t^c$ produced by the content system at the current step, yielding the gated weighting $\textbf{w}_t^g$:
\begin{equation}
\textbf{w}_t^g \leftarrow g_t\textbf{w}_t^c + (1-g_t)\textbf{w}_{t-1}
\end{equation}
\noindent
The main aim of having this module is to give the addressing mechanism capability to completely neglect the weighting produced by content addressing based module and use the previous final weighting as a reference point.
This gives NTM the power to iterate over memory slots (this module just sets the reference point i.e. the weighting to start from while the next module takes care of the iteration part). One can easily observe that, if the gate is zero, the the content weighting is entirely ignored and weighting from previous time step is used and if the gate is one then the weighting from the previous iteration is ignored, and the system applies content based addressing. Note that the derivative of $\textbf{w}_t^g$ with respect to $g_t$, $\textbf{w}_t^c$ and $\textbf{w}_{t-1}$ are:
\begin{equation}
\frac{\partial w_t^g(i)}{\partial g_t} \leftarrow w_t^c(i)-w_{t-1}(i)
\end{equation}

\begin{equation}
\frac{\partial w_t^g(i)}{\partial w_t^c(j)} \leftarrow g_t\mathbb{I}(i=j)
\end{equation}

\begin{equation}
\frac{\partial w_t^g(i)}{\partial w_{t-1}(j)} \leftarrow (1-g_t)\mathbb{I}(i=j)
\end{equation}

\subsubsection{Convolutional Shift}
The controller emits a shift weighting $s_t$ that defines a normalised distribution over allowed integer shifts. If we index \textit{N} memory locations from 0 to \textit{N}-1, the rotation applied to $\textbf{w}_t^g$ by $\textbf{s}_t$ can be expressed as the following circular convolution.
\begin{equation}
\widetilde{w}_t(i) \leftarrow \sum_{j=0}^{N-1} w_t^g(j)s_t(i-j)
\end{equation}
\noindent
As an example, suppose memory looks like ${[A,B,C,D,...,Z]}$, 26 memory slots with one unique alphabetic character each and the weighting produced by interpolation module is ${[0,1,0,0,0,...,0]}$ (focusing on memory slot containing ${B}$). Now, suppose that shift weighting vector produced only allows shifts of 1 and the corresponding shift weight value be 1. That means $\textbf{s}_t(1)=1$ and $\textbf{s}_t(i)=0$ $\forall$ i $\neq$ 1. Then, using the formula new weighting produced will be ${[0,0,1,0,...,0]}$. Also, note that if the shift weighting vector produced, only allows shifts of -1, 0 and 1 and the corresponding shift weight value be 0.1, 0.8 and 0.1 respectively. That means $\textbf{s}_t(1)=0.1$, $\textbf{s}_t(-1)=0.1$, st(0)=0.8 and $\textbf{s}_t(i)=0$ $\forall$ i $\neq$ to \{-1,0,1\}. Then, the new weighting will be ${[0.1,0.8,0.1,0,...,0]}$ which is slightly blurred over three points. To combat this we require sharpening. Note that the derivative of $\widetilde{\textbf{w}_t}$ with respect to $\textbf{s}_t$ and $\textbf{w}_t^g$ are (note that the subtraction is modulo N): 
\begin{equation}
\frac{\partial \widetilde{w}_t(i)}{\partial s_t(k)} \leftarrow \sum_{k=0}^{N-1} w_t^g(i-k)
\end{equation}
\begin{equation}
\frac{\partial \widetilde{w}_t(i)}{\partial w_t^g(j)} \leftarrow \sum_{j=0}^{N-1} s_t(i-j)
\end{equation}

\subsubsection{Sharpening}
Each head emits one further scalar $\gamma_t$ whose effect is to sharpen the weights as follows:
\begin{equation}
w_t(i) \leftarrow \frac{\widetilde{w}_t(i)^{\gamma_t}}{\sum_{j} \widetilde{w}_t(j)^{\gamma_t}}
\end{equation}
\noindent
Note that the derivative of $\textbf{w}_t$ with respect to $\widetilde{\textbf{w}_t}$ and $\gamma_t$ are:
\begin{equation}
\frac{\partial w_t(i)}{\partial \gamma_t} \leftarrow \frac{\widetilde{w}_t(i)^\gamma_t}{\sum_{j}\widetilde{w}_t(j)^\gamma_t}\left [ \log(\widetilde{w}_t(i)) - \frac{\sum_{j} \log(\widetilde{w}_t(j))\widetilde{w}_t(j)^\gamma_t}{\sum_{k}\widetilde{w}_t(k)^\gamma_t} \right ]
\end{equation}

\begin{equation}
\frac{\partial w_t(i)}{\partial \widetilde{w}_t(i)} \leftarrow \frac{\gamma_t\widetilde{w}_t(i)^{\gamma_t-1}}{\sum_{j}\widetilde{w}_t(j)^\gamma_t}\left [ 1 - \frac{\widetilde{w}_t(i)^\gamma_t}{\sum_{k}\widetilde{w}_t(k)^\gamma_t} \right ]
\end{equation}

\noindent
and when i $\neq$ j

\begin{equation}
\frac{\partial w_t(i)}{\partial \widetilde{w}_t(j)} \leftarrow \frac{-\gamma_t\widetilde{w}_t(i)^{\gamma_t}\widetilde{w}_t(j)^{\gamma_t-1}}{\sum_{k}\widetilde{w}_t(k)^\gamma_t}
\end{equation}

\noindent
The combined addressing system of weighting interpolation and content and location-based addressing can operate in three complementary modes:

\noindent
1. A weighting can be chosen by the content system without any modification by the location system.

\noindent
2. A weighting produced by the content addressing system can be chosen and then shifted. This allows the focus to jump to a location next to, but not on, an address accessed by content; in computational terms this allows a head to find a contiguous block of data, then access a particular element within that block.

\noindent
3. A weighting from the previous time step can be rotated without any input from the content-based addressing system. This allows the weighting to iterate through a sequence of addresses by advancing the same distance at each time-step.

\section{Controller}
The choices for the controller include LSTM\cite{lstm}, feedforward networks and recurrent network. We have used feedforward networks as unfolded recurrent networks for explaining the learning procedure and in our experimental analysis as well.

\section{Training Neural Turing Machine}
\section{Experiments}
\subsection{Copy}
\subsection{POS-Tagging}